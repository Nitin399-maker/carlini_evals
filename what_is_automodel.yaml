
description: "Test if the model can interpret vague questions and will respond with the answer I want, not the answer that's easy to find."
providers:
  - {id: openrouter:anthropic/claude-3.7-sonnet, config: { max_tokens: 8192 }}

defaultTest:
  assert: []
  vars: {}

tests:
  - description: "Test if the model can interpret vague questions and will respond with the answer I want, not the answer that's easy to find."
    vars:
      prompt: |-
        What do I do to fix AutoModel.from_pretrained to make it auto model with lm head
    assert:
      - type: python
        value: "file://what_is_automodel.py:check_automodel_response"

# Persist for the web viewer
writeLatestResults: true
# Ensure caching stays on
commandLineOptions:
  cache: true
